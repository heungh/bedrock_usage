import streamlit as st
import boto3
import pandas as pd
from datetime import datetime, timedelta
import time
from typing import Dict, List
import logging
import os
from pathlib import Path


# ë¡œê¹… ì„¤ì •
def setup_logger():
    """ë””ë²„ê¹…ìš© ë¡œê±° ì„¤ì •"""
    log_dir = Path(__file__).parent / "log"
    log_dir.mkdir(exist_ok=True)

    log_filename = (
        log_dir / f"bedrock_tracker_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )

    logger = logging.getLogger("BedrockTracker")
    logger.setLevel(logging.DEBUG)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_filename, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
    )
    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.info(f"Logger initialized. Log file: {log_filename}")

    return logger


# ê¸€ë¡œë²Œ ë¡œê±°
logger = setup_logger()

# AWS Bedrock ëª¨ë¸ ê°€ê²© í…Œì´ë¸” (ëª¨ë“  ë¦¬ì „ ë™ì¼)
MODEL_PRICING = {
    # Claude 3 ëª¨ë¸
    "claude-3-haiku-20240307": {
        "input": 0.00025 / 1000,  # per token
        "output": 0.00125 / 1000,
    },
    "claude-3-sonnet-20240229": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    "claude-3-opus-20240229": {"input": 0.015 / 1000, "output": 0.075 / 1000},
    # Claude 3.5 ëª¨ë¸
    "claude-3-5-haiku-20241022": {"input": 0.0008 / 1000, "output": 0.004 / 1000},
    "claude-3-5-sonnet-20240620": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    "claude-3-5-sonnet-20241022": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    # Claude 3.7 ëª¨ë¸
    "claude-3-7-sonnet-20250219": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    # Claude 4 ëª¨ë¸
    "claude-sonnet-4-20250514": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    "claude-sonnet-4-5-20250929": {"input": 0.003 / 1000, "output": 0.015 / 1000},
    "claude-opus-4-20250514": {"input": 0.015 / 1000, "output": 0.075 / 1000},
    "claude-opus-4-1-20250808": {"input": 0.015 / 1000, "output": 0.075 / 1000},
}

# ë¦¬ì „ ì„¤ì •
REGIONS = {
    "us-east-1": "US East (N. Virginia)",
    "us-west-2": "US West (Oregon)",
    "eu-central-1": "Europe (Frankfurt)",
    "ap-northeast-1": "Asia Pacific (Tokyo)",
    "ap-northeast-2": "Asia Pacific (Seoul)",
    "ap-southeast-1": "Asia Pacific (Singapore)",
}

default_region = "us-east-1"


def get_model_cost(model_id: str, input_tokens: int, output_tokens: int) -> float:
    """ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚°"""
    logger.debug(
        f"Calculating cost for model: {model_id}, input: {input_tokens}, output: {output_tokens}"
    )

    # ëª¨ë¸ IDì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0 -> claude-3-haiku-20240307)
    model_name = model_id.split(".")[-1].split("-v")[0] if "." in model_id else model_id

    # ê°€ê²© í…Œì´ë¸”ì—ì„œ ì°¾ê¸°
    for key, pricing in MODEL_PRICING.items():
        if key in model_name:
            cost = (input_tokens * pricing["input"]) + (
                output_tokens * pricing["output"]
            )
            logger.debug(f"Model: {key}, Cost: ${cost:.6f}")
            return cost

    # ê¸°ë³¸ ê°€ê²© (Claude 3 Haiku)
    logger.warning(f"Unknown model: {model_id}, using default pricing")
    default_cost = (input_tokens * 0.00025 / 1000) + (output_tokens * 0.00125 / 1000)
    return default_cost


class BedrockAthenaTracker:
    def __init__(self, region=default_region):
        logger.info(f"Initializing BedrockAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        # STS í´ë¼ì´ì–¸íŠ¸ë„ regionì„ ì§€ì •í•˜ì—¬ ìƒì„±
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        # ë¦¬ì „ë³„ Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚·
        self.results_bucket = f"bedrock-analytics-{self.account_id}-{self.region}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def get_current_logging_config(self) -> Dict:
        """í˜„ì¬ ì„¤ì •ëœ Model Invocation Logging ì •ë³´ ì¡°íšŒ"""
        logger.info("Getting current logging configuration")
        try:
            bedrock = boto3.client("bedrock", region_name=self.region)
            response = bedrock.get_model_invocation_logging_configuration()

            if "loggingConfig" in response:
                config = response["loggingConfig"]

                if "s3Config" in config:
                    result = {
                        "type": "s3",
                        "bucket": config["s3Config"].get("bucketName", ""),
                        "prefix": config["s3Config"].get("keyPrefix", ""),
                        "status": "enabled",
                    }
                    logger.info(f"Logging config: {result}")
                    return result

            logger.warning("Logging is disabled")
            return {"status": "disabled"}

        except Exception as e:
            logger.error(f"Error getting logging config: {str(e)}")
            return {"status": "error", "error": str(e)}

    def set_results_bucket(self, bucket_name: str):
        """Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚· ì„¤ì •"""
        self.results_bucket = bucket_name
        logger.info(f"Results bucket set to: {self.results_bucket}")

    def execute_athena_query(
        self, query: str, database: str = "bedrock_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            st.error(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def get_user_cost_analysis(
        self, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ë¹„ìš© ë¶„ì„"""
        logger.info(f"Getting user cost analysis from {start_date} to {end_date}")

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        GROUP BY identity.arn
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_user_app_detail_analysis(
        self, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„"""
        logger.info(f"Getting user-app detail analysis from {start_date} to {end_date}")

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        GROUP BY identity.arn, modelId
        ORDER BY user_or_app, call_count DESC
        """

        return self.execute_athena_query(query)

    def get_hourly_usage_pattern(
        self, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """ì‹œê°„ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(f"Getting hourly usage pattern from {start_date} to {end_date}")

        query = f"""
        SELECT
            year, month, day, hour,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        GROUP BY year, month, day, hour
        ORDER BY year, month, day, hour
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(f"Getting daily usage pattern from {start_date} to {end_date}")

        query = f"""
        SELECT
            year, month, day,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        GROUP BY year, month, day
        ORDER BY year, month, day
        """

        return self.execute_athena_query(query)

    def get_model_usage_stats(
        self, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„"""
        logger.info(f"Getting model usage stats from {start_date} to {end_date}")

        query = f"""
        SELECT
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            AVG(CAST(input.inputTokenCount AS DOUBLE)) as avg_input_tokens,
            AVG(CAST(output.outputTokenCount AS DOUBLE)) as avg_output_tokens,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        GROUP BY modelId
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_total_summary(self, start_date: datetime, end_date: datetime) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„"""
        logger.info(f"Getting total summary from {start_date} to {end_date}")

        query = f"""
        SELECT
            COUNT(*) as total_calls,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE year >= '{start_date.year}'
            AND month >= '{start_date.month:02d}'
            AND day >= '{start_date.day:02d}'
            AND year <= '{end_date.year}'
            AND month <= '{end_date.month:02d}'
            AND day <= '{end_date.day:02d}'
        """

        df = self.execute_athena_query(query)

        if not df.empty:
            result = {
                "total_calls": (
                    int(df.iloc[0]["total_calls"]) if df.iloc[0]["total_calls"] else 0
                ),
                "total_input_tokens": (
                    int(df.iloc[0]["total_input_tokens"])
                    if df.iloc[0]["total_input_tokens"]
                    else 0
                ),
                "total_output_tokens": (
                    int(df.iloc[0]["total_output_tokens"])
                    if df.iloc[0]["total_output_tokens"]
                    else 0
                ),
                "total_cost_usd": 0.0,  # ëª¨ë¸ë³„ë¡œ ê³„ì‚° í•„ìš”
            }
            logger.info(f"Total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "total_calls": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost_usd": 0.0,
            }


def calculate_cost_for_dataframe(
    df: pd.DataFrame, model_col: str = "model_name"
) -> pd.DataFrame:
    """DataFrameì— ë¹„ìš© ì»¬ëŸ¼ ì¶”ê°€"""
    logger.info(f"Calculating cost for DataFrame with {len(df)} rows")

    if df.empty:
        return df

    costs = []
    for _, row in df.iterrows():
        model = row.get(model_col, "")
        input_tokens = (
            int(row.get("total_input_tokens", 0))
            if row.get("total_input_tokens")
            else 0
        )
        output_tokens = (
            int(row.get("total_output_tokens", 0))
            if row.get("total_output_tokens")
            else 0
        )
        cost = get_model_cost(model, input_tokens, output_tokens)
        costs.append(cost)

    df["estimated_cost_usd"] = costs
    logger.info(f"Total cost calculated: ${sum(costs):.4f}")
    return df


def main():
    logger.info("Starting Bedrock Analytics Dashboard")

    st.set_page_config(
        page_title="Bedrock Analytics Dashboard", page_icon="ğŸ“Š", layout="wide"
    )

    st.title("ğŸ“Š AWS Bedrock Analytics Dashboard")
    st.markdown("**Athena ê¸°ë°˜ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ ë¶„ì„**")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

    # ë¦¬ì „ ì„ íƒ
    selected_region = st.sidebar.selectbox(
        "ë¦¬ì „ ì„ íƒ",
        options=list(REGIONS.keys()),
        format_func=lambda x: f"{x} - {REGIONS[x]}",
        index=4,
    )
    logger.info(f"Selected region: {selected_region}")

    # ë‚ ì§œ ë²”ìœ„ ì„ íƒ
    st.sidebar.subheader("ğŸ“… ë‚ ì§œ ë²”ìœ„ ì„ íƒ")

    col1, col2 = st.sidebar.columns(2)
    with col1:
        start_date = st.date_input(
            "ì‹œì‘ ë‚ ì§œ",
            value=datetime.now() - timedelta(days=7),
            max_value=datetime.now(),
        )
    with col2:
        end_date = st.date_input(
            "ì¢…ë£Œ ë‚ ì§œ", value=datetime.now(), max_value=datetime.now()
        )

    logger.info(f"Date range: {start_date} to {end_date}")

    # í˜„ì¬ ë¡œê¹… ì„¤ì • ìë™ ì¡°íšŒ
    tracker = BedrockAthenaTracker(region=selected_region)

    with st.spinner("í˜„ì¬ Model Invocation Logging ì„¤ì • í™•ì¸ ì¤‘..."):
        current_config = tracker.get_current_logging_config()

    # ì„¤ì • ìƒíƒœ í‘œì‹œ
    if current_config["status"] == "enabled":
        st.success("âœ… Model Invocation Loggingì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")

        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ“ **S3 ë²„í‚·**: ì„¤ì •ë¨ ({selected_region})")
        with col2:
            st.info(f"ğŸ“‚ **í”„ë¦¬í”½ìŠ¤**: ì„¤ì •ë¨")

    elif current_config["status"] == "disabled":
        st.error("âŒ Model Invocation Loggingì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        st.markdown("ğŸ‘‡ ë¨¼ì € ì„¤ì •ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”:")
        st.code("python setup_bedrock_analytics.py")
        logger.error("Model Invocation Logging is disabled")
        return

    else:
        st.warning(
            f"âš ï¸ ì„¤ì • í™•ì¸ ì¤‘ ì˜¤ë¥˜: {current_config.get('error', 'Unknown error')}"
        )
        logger.error(f"Error checking logging config: {current_config.get('error')}")

    # ë¶„ì„ ì‹¤í–‰
    if st.sidebar.button("ğŸ” ë°ì´í„° ë¶„ì„", type="primary"):
        logger.info("Analysis button clicked")

        with st.spinner("Athenaì—ì„œ ë°ì´í„° ë¶„ì„ ì¤‘..."):

            # ì „ì²´ ìš”ì•½
            summary = tracker.get_total_summary(start_date, end_date)

            st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ì´ API í˜¸ì¶œ", f"{summary['total_calls']:,}")

            with col2:
                st.metric("ì´ Input í† í°", f"{summary['total_input_tokens']:,}")

            with col3:
                st.metric("ì´ Output í† í°", f"{summary['total_output_tokens']:,}")

            # ëª¨ë¸ë³„ í†µê³„ë¡œ ì´ ë¹„ìš© ê³„ì‚°
            model_df = tracker.get_model_usage_stats(start_date, end_date)
            if not model_df.empty:
                model_df = calculate_cost_for_dataframe(model_df)
                total_cost = model_df["estimated_cost_usd"].sum()
                summary["total_cost_usd"] = total_cost

            with col4:
                st.metric("ì´ ë¹„ìš©", f"${summary['total_cost_usd']:.4f}")

            # ì‚¬ìš©ìë³„ ë¶„ì„
            st.header("ğŸ‘¥ ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¶„ì„")

            user_df = tracker.get_user_cost_analysis(start_date, end_date)

            if not user_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(
                            user_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚°ì„ ìœ„í•œ ì„ì‹œ ëª¨ë¸ëª… ì¶”ê°€ (ëª¨ë¸ë³„ í‰ê·  ì‚¬ìš©)
                # ì‹¤ì œë¡œëŠ” ê° ì‚¬ìš©ìê°€ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ì•Œì•„ì•¼ ì •í™•í•¨
                # ì—¬ê¸°ì„œëŠ” Claude 3 Haiku ê¸°ë³¸ ê°€ê²© ì‚¬ìš©
                user_df["estimated_cost_usd"] = (
                    user_df["total_input_tokens"] * 0.00025 / 1000
                    + user_df["total_output_tokens"] * 0.00125 / 1000
                )

                st.dataframe(user_df, use_container_width=True)

                # ë¹„ìš© ì°¨íŠ¸
                if len(user_df) > 0:
                    import plotly.express as px

                    fig = px.bar(
                        user_df.head(10),
                        x="user_or_app",
                        y="estimated_cost_usd",
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¹„ìš©",
                        labels={
                            "user_or_app": "ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜",
                            "estimated_cost_usd": "ë¹„ìš© (USD)",
                        },
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„
            st.header("ğŸ“± ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„")

            user_app_df = tracker.get_user_app_detail_analysis(start_date, end_date)

            if not user_app_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_app_df.columns:
                        user_app_df[col] = pd.to_numeric(
                            user_app_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚°
                user_app_df = calculate_cost_for_dataframe(user_app_df)

                st.dataframe(user_app_df, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ëª¨ë¸ë³„ ë¶„ì„
            st.header("ğŸ¤– ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„")

            if not model_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "avg_input_tokens",
                    "avg_output_tokens",
                    "total_input_tokens",
                    "total_output_tokens",
                    "estimated_cost_usd",
                ]
                for col in numeric_columns:
                    if col in model_df.columns:
                        model_df[col] = pd.to_numeric(
                            model_df[col], errors="coerce"
                        ).fillna(0)

                st.dataframe(model_df, use_container_width=True)

                # ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨ ì°¨íŠ¸
                if len(model_df) > 0:
                    import plotly.express as px

                    fig = px.pie(
                        model_df,
                        values="call_count",
                        names="model_name",
                        title="ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨",
                    )
                    st.plotly_chart(fig, use_container_width=True)

            # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
            st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

            daily_df = tracker.get_daily_usage_pattern(start_date, end_date)

            if not daily_df.empty and len(daily_df) > 0:
                # ë‚ ì§œ ì»¬ëŸ¼ ìƒì„±
                daily_df["date"] = pd.to_datetime(
                    daily_df["year"]
                    + "-"
                    + daily_df["month"].str.zfill(2)
                    + "-"
                    + daily_df["day"].str.zfill(2)
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                daily_df["call_count"] = pd.to_numeric(
                    daily_df["call_count"], errors="coerce"
                ).fillna(0)
                daily_df["total_input_tokens"] = pd.to_numeric(
                    daily_df["total_input_tokens"], errors="coerce"
                ).fillna(0)
                daily_df["total_output_tokens"] = pd.to_numeric(
                    daily_df["total_output_tokens"], errors="coerce"
                ).fillna(0)

                import plotly.express as px

                fig = px.line(
                    daily_df,
                    x="date",
                    y="call_count",
                    title="ì¼ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"date": "ë‚ ì§œ", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰
                fig2 = px.line(
                    daily_df,
                    x="date",
                    y=["total_input_tokens", "total_output_tokens"],
                    title="ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰",
                    labels={
                        "date": "ë‚ ì§œ",
                        "value": "í† í° ìˆ˜",
                        "variable": "í† í° ì¢…ë¥˜",
                    },
                )
                st.plotly_chart(fig2, use_container_width=True)

            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            st.header("â° ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´")

            hourly_df = tracker.get_hourly_usage_pattern(start_date, end_date)

            if not hourly_df.empty and len(hourly_df) > 0:
                # ì‹œê°„ ì»¬ëŸ¼ ìƒì„±
                hourly_df["datetime"] = pd.to_datetime(
                    hourly_df["year"]
                    + "-"
                    + hourly_df["month"].str.zfill(2)
                    + "-"
                    + hourly_df["day"].str.zfill(2)
                    + " "
                    + hourly_df["hour"].str.zfill(2)
                    + ":00:00"
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                hourly_df["call_count"] = pd.to_numeric(
                    hourly_df["call_count"], errors="coerce"
                ).fillna(0)

                import plotly.express as px

                fig = px.line(
                    hourly_df,
                    x="datetime",
                    y="call_count",
                    title="ì‹œê°„ëŒ€ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"datetime": "ì‹œê°„", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                )
                st.plotly_chart(fig, use_container_width=True)

    else:
        # ì´ˆê¸° í™”ë©´
        st.info(
            "ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¦¬ì „ê³¼ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•œ í›„ 'ë°ì´í„° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
        )

        st.markdown("### ğŸš€ Athena ê¸°ë°˜ ë¶„ì„ì˜ ì¥ì ")

        st.markdown(
            """
        - **ğŸš€ ê³ ì„±ëŠ¥**: í˜íƒ€ë°”ì´íŠ¸ ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
        - **ğŸ’° ë¹„ìš© íš¨ìœ¨ì **: ìŠ¤ìº”í•œ ë°ì´í„°ëŸ‰ë§Œí¼ë§Œ ê³¼ê¸ˆ
        - **ğŸ“Š SQL ì¿¼ë¦¬**: í‘œì¤€ SQLë¡œ ë³µì¡í•œ ë¶„ì„ ê°€ëŠ¥
        - **ğŸ”— QuickSight ì—°ë™**: ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ê°€ëŠ¥
        - **ğŸ“ˆ í™•ì¥ì„±**: ë°ì´í„° ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ ì—†ìŒ
        """
        )

        st.markdown("### ğŸ› ï¸ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš°")

        st.code(
            """
# Bedrock Analytics í†µí•© ì„¤ì • ì‹¤í–‰
python setup_bedrock_analytics.py

# ë˜ëŠ” ì»¤ìŠ¤í…€ ë²„í‚·ëª…ìœ¼ë¡œ ì„¤ì •
python setup_bedrock_analytics.py --bucket my-bedrock-logs
        """
        )

        st.markdown("### ğŸ“‹ ì§€ì› ëª¨ë¸")
        st.markdown(
            """
        - **Claude 3**: Haiku, Sonnet, Opus
        - **Claude 3.5**: Haiku, Sonnet
        - **Claude 3.7**: Sonnet
        - **Claude 4**: Sonnet 4, Sonnet 4.5
        - **Claude 4.1**: Opus
        """
        )

        st.markdown("### ğŸŒ ì§€ì› ë¦¬ì „")
        for region_id, region_name in REGIONS.items():
            st.markdown(f"- **{region_id}**: {region_name}")

    logger.info("Dashboard rendering complete")


if __name__ == "__main__":
    main()
