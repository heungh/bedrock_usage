import streamlit as st
import boto3
import pandas as pd
from datetime import datetime, timedelta
import time
from typing import Dict, List
import logging
import os
from pathlib import Path


# ë¡œê¹… ì„¤ì •
def setup_logger():
    """ë””ë²„ê¹…ìš© ë¡œê±° ì„¤ì •"""
    log_dir = Path(__file__).parent / "log"
    log_dir.mkdir(exist_ok=True)

    log_filename = (
        log_dir / f"bedrock_tracker_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    )

    logger = logging.getLogger("BedrockTracker")
    logger.setLevel(logging.DEBUG)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_filename, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)

    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
    )
    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.info(f"Logger initialized. Log file: {log_filename}")

    return logger


# ê¸€ë¡œë²Œ ë¡œê±°
logger = setup_logger()

# AWS Bedrock ëª¨ë¸ ê°€ê²© í…Œì´ë¸” (ë¦¬ì „ë³„)
# ì°¸ê³ : ìµœì‹  ê°€ê²©ì€ https://aws.amazon.com/bedrock/pricing/ ì—ì„œ í™•ì¸í•˜ì„¸ìš”
# ê°€ê²©ì€ USD ê¸°ì¤€ì´ë©°, 1000 í† í°ë‹¹ ê°€ê²©ì…ë‹ˆë‹¤
MODEL_PRICING = {
    # ê¸°ë³¸ ê°€ê²© (ëŒ€ë¶€ë¶„ì˜ ë¦¬ì „ì— ì ìš©)
    "default": {
        # Claude 3 ëª¨ë¸
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        # Claude 3.5 ëª¨ë¸
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        # Claude 3.7 ëª¨ë¸
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        # Claude 4 ëª¨ë¸
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US East (N. Virginia) - us-east-1
    "us-east-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # US West (Oregon) - us-west-2
    "us-west-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Europe (Frankfurt) - eu-central-1
    "eu-central-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Tokyo) - ap-northeast-1
    "ap-northeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Seoul) - ap-northeast-2
    "ap-northeast-2": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
    # Asia Pacific (Singapore) - ap-southeast-1
    "ap-southeast-1": {
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-5-haiku-20241022": {"input": 0.0008, "output": 0.004},
        "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015},
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-7-sonnet-20250219": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
        "claude-sonnet-4-5-20250929": {"input": 0.003, "output": 0.015},
        "claude-opus-4-20250514": {"input": 0.015, "output": 0.075},
        "claude-opus-4-1-20250808": {"input": 0.015, "output": 0.075},
    },
}

# ë¦¬ì „ ì„¤ì •
REGIONS = {
    "us-east-1": "US East (N. Virginia)",
    "us-west-2": "US West (Oregon)",
    "eu-central-1": "Europe (Frankfurt)",
    "ap-northeast-1": "Asia Pacific (Tokyo)",
    "ap-northeast-2": "Asia Pacific (Seoul)",
    "ap-southeast-1": "Asia Pacific (Singapore)",
}

default_region = "us-east-1"


def get_model_cost(model_id: str, input_tokens: int, output_tokens: int, region: str = "default") -> float:
    """ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚° (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        model_id: Bedrock ëª¨ë¸ ID (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0)
        input_tokens: ì…ë ¥ í† í° ìˆ˜
        output_tokens: ì¶œë ¥ í† í° ìˆ˜
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        float: ê³„ì‚°ëœ ë¹„ìš© (USD)
    """
    logger.debug(
        f"Calculating cost for model: {model_id}, input: {input_tokens}, output: {output_tokens}, region: {region}"
    )

    # ëª¨ë¸ IDì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ì˜ˆ: us.anthropic.claude-3-haiku-20240307-v1:0 -> claude-3-haiku-20240307)
    model_name = model_id.split(".")[-1].split("-v")[0] if "." in model_id else model_id

    # ë¦¬ì „ë³„ ê°€ê²© í…Œì´ë¸” ì„ íƒ (í•´ë‹¹ ë¦¬ì „ì´ ì—†ìœ¼ë©´ default ì‚¬ìš©)
    region_pricing = MODEL_PRICING.get(region, MODEL_PRICING["default"])

    # ê°€ê²© í…Œì´ë¸”ì—ì„œ ëª¨ë¸ ì°¾ê¸°
    for key, pricing in region_pricing.items():
        if key in model_name:
            # ê°€ê²©ì€ 1000 í† í°ë‹¹ ê°€ê²©ì´ë¯€ë¡œ 1000ìœ¼ë¡œ ë‚˜ëˆ”
            cost = (input_tokens * pricing["input"] / 1000) + (
                output_tokens * pricing["output"] / 1000
            )
            logger.debug(f"Model: {key}, Region: {region}, Cost: ${cost:.6f}")
            return cost

    # ê¸°ë³¸ ê°€ê²© (Claude 3 Haiku)
    logger.warning(f"Unknown model: {model_id}, using default pricing (Claude 3 Haiku)")
    default_pricing = MODEL_PRICING["default"]["claude-3-haiku-20240307"]
    default_cost = (input_tokens * default_pricing["input"] / 1000) + (
        output_tokens * default_pricing["output"] / 1000
    )
    return default_cost


class BedrockAthenaTracker:
    def __init__(self, region=default_region):
        logger.info(f"Initializing BedrockAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        # STS í´ë¼ì´ì–¸íŠ¸ë„ regionì„ ì§€ì •í•˜ì—¬ ìƒì„±
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        # ë¦¬ì „ë³„ Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚·
        self.results_bucket = f"bedrock-analytics-{self.account_id}-{self.region}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def get_current_logging_config(self) -> Dict:
        """í˜„ì¬ ì„¤ì •ëœ Model Invocation Logging ì •ë³´ ì¡°íšŒ"""
        logger.info("Getting current logging configuration")
        try:
            bedrock = boto3.client("bedrock", region_name=self.region)
            response = bedrock.get_model_invocation_logging_configuration()

            if "loggingConfig" in response:
                config = response["loggingConfig"]

                if "s3Config" in config:
                    result = {
                        "type": "s3",
                        "bucket": config["s3Config"].get("bucketName", ""),
                        "prefix": config["s3Config"].get("keyPrefix", ""),
                        "status": "enabled",
                    }
                    logger.info(f"Logging config: {result}")
                    return result

            logger.warning("Logging is disabled")
            return {"status": "disabled"}

        except Exception as e:
            logger.error(f"Error getting logging config: {str(e)}")
            return {"status": "error", "error": str(e)}

    def set_results_bucket(self, bucket_name: str):
        """Athena ê²°ê³¼ ì €ì¥ìš© ë²„í‚· ì„¤ì •"""
        self.results_bucket = bucket_name
        logger.info(f"Results bucket set to: {self.results_bucket}")

    def execute_athena_query(
        self, query: str, database: str = "bedrock_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            st.error(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def get_user_cost_analysis(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ë¹„ìš© ë¶„ì„"""
        logger.info(f"Getting user cost analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_user_app_detail_analysis(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„"""
        logger.info(f"Getting user-app detail analysis from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            CASE
                WHEN identity.arn LIKE '%assumed-role%' THEN
                    regexp_extract(identity.arn, 'assumed-role/([^/]+)')
                WHEN identity.arn LIKE '%user%' THEN
                    regexp_extract(identity.arn, 'user/([^/]+)')
                ELSE 'Unknown'
            END as user_or_app,
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY identity.arn, modelId
        ORDER BY user_or_app, call_count DESC
        """

        return self.execute_athena_query(query)

    def get_hourly_usage_pattern(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì‹œê°„ë³„ ì‚¬ìš© íŒ¨í„´ - timestampì—ì„œ hour ì¶”ì¶œ"""
        logger.info(f"Getting hourly usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year,
            month,
            day,
            date_format(from_iso8601_timestamp(timestamp), '%H') as hour,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        ORDER BY year, month, day, date_format(from_iso8601_timestamp(timestamp), '%H')
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(f"Getting daily usage pattern from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            year, month, day,
            COUNT(*) as call_count,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY year, month, day
        ORDER BY year, month, day
        """

        return self.execute_athena_query(query)

    def get_model_usage_stats(
        self, start_date: datetime, end_date: datetime, arn_pattern: str = None
    ) -> pd.DataFrame:
        """ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„"""
        logger.info(f"Getting model usage stats from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            regexp_extract(modelId, '([^/]+)$') as model_name,
            COUNT(*) as call_count,
            AVG(CAST(input.inputTokenCount AS DOUBLE)) as avg_input_tokens,
            AVG(CAST(output.outputTokenCount AS DOUBLE)) as avg_output_tokens,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        GROUP BY modelId
        ORDER BY call_count DESC
        """

        return self.execute_athena_query(query)

    def get_total_summary(self, start_date: datetime, end_date: datetime, arn_pattern: str = None) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„"""
        logger.info(f"Getting total summary from {start_date} to {end_date}, arn_pattern={arn_pattern}")

        arn_filter = f"AND identity.arn LIKE '%{arn_pattern}%'" if arn_pattern else ""

        query = f"""
        SELECT
            COUNT(*) as total_calls,
            SUM(CAST(input.inputTokenCount AS BIGINT)) as total_input_tokens,
            SUM(CAST(output.outputTokenCount AS BIGINT)) as total_output_tokens
        FROM bedrock_invocation_logs
        WHERE CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-', LPAD(day, 2, '0')) AS DATE)
            BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}' AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {arn_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty:
            result = {
                "total_calls": (
                    int(df.iloc[0]["total_calls"]) if df.iloc[0]["total_calls"] else 0
                ),
                "total_input_tokens": (
                    int(df.iloc[0]["total_input_tokens"])
                    if df.iloc[0]["total_input_tokens"]
                    else 0
                ),
                "total_output_tokens": (
                    int(df.iloc[0]["total_output_tokens"])
                    if df.iloc[0]["total_output_tokens"]
                    else 0
                ),
                "total_cost_usd": 0.0,  # ëª¨ë¸ë³„ë¡œ ê³„ì‚° í•„ìš”
            }
            logger.info(f"Total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "total_calls": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost_usd": 0.0,
            }


class QCliAthenaTracker:
    """Amazon Q CLI ì‚¬ìš©ëŸ‰ ì¶”ì ì„ ìœ„í•œ Athena ì¿¼ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, region=default_region):
        logger.info(f"Initializing QCliAthenaTracker with region: {region}")
        self.region = region
        self.athena = boto3.client("athena", region_name=region)
        sts_client = boto3.client("sts", region_name=region)
        self.account_id = sts_client.get_caller_identity()["Account"]
        self.results_bucket = f"amazonq-developer-reports-{self.account_id}"
        logger.info(
            f"Account ID: {self.account_id}, Results bucket: {self.results_bucket}"
        )

    def execute_athena_query(
        self, query: str, database: str = "qcli_analytics"
    ) -> pd.DataFrame:
        """Athena ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        logger.info(f"Executing Athena query on database: {database}")
        logger.debug(f"Query: {query}")

        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena.start_query_execution(
                QueryString=query,
                QueryExecutionContext={"Database": database},
                ResultConfiguration={
                    "OutputLocation": f"s3://{self.results_bucket}/query-results/"
                },
            )

            query_id = response["QueryExecutionId"]
            logger.info(f"Query execution started: {query_id}")

            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait = 60
            for i in range(max_wait):
                result = self.athena.get_query_execution(QueryExecutionId=query_id)
                status = result["QueryExecution"]["Status"]["State"]

                if status == "SUCCEEDED":
                    logger.info(f"Query succeeded in {i+1} seconds")
                    break
                elif status in ["FAILED", "CANCELLED"]:
                    error = result["QueryExecution"]["Status"].get(
                        "StateChangeReason", "Unknown error"
                    )
                    logger.error(f"Query failed: {error}")
                    raise Exception(f"Query failed: {error}")

                time.sleep(1)
            else:
                logger.error("Query timeout")
                raise Exception("Query timeout")

            # ê²°ê³¼ ì¡°íšŒ
            result_response = self.athena.get_query_results(QueryExecutionId=query_id)

            # DataFrameìœ¼ë¡œ ë³€í™˜
            columns = [
                col["Label"]
                for col in result_response["ResultSet"]["ResultSetMetadata"][
                    "ColumnInfo"
                ]
            ]
            rows = []

            for row in result_response["ResultSet"]["Rows"][1:]:  # í—¤ë” ì œì™¸
                row_data = [field.get("VarCharValue", "") for field in row["Data"]]
                rows.append(row_data)

            df = pd.DataFrame(rows, columns=columns)
            logger.info(f"Query returned {len(df)} rows")
            return df

        except Exception as e:
            logger.error(f"Athena query execution failed: {str(e)}")
            st.error(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def get_total_summary(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> Dict:
        """ì „ì²´ ìš”ì•½ í†µê³„ - Amazon Q Developer CSV ë¦¬í¬íŠ¸ ê¸°ë°˜"""
        logger.info(
            f"Getting QCli total summary from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND user_id LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            SUM(CAST(request_count AS BIGINT)) as total_requests,
            SUM(CAST(agentic_request_count AS BIGINT)) as total_agentic_requests,
            SUM(CAST(cli_request_count AS BIGINT)) as total_cli_requests,
            COUNT(DISTINCT user_id) as unique_users,
            COUNT(DISTINCT date) as active_days
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        """

        df = self.execute_athena_query(query)

        if not df.empty and df.iloc[0]["total_requests"]:
            result = {
                "total_requests": (
                    int(df.iloc[0]["total_requests"])
                    if df.iloc[0]["total_requests"]
                    else 0
                ),
                "total_agentic_requests": (
                    int(df.iloc[0]["total_agentic_requests"])
                    if df.iloc[0]["total_agentic_requests"]
                    else 0
                ),
                "total_cli_requests": (
                    int(df.iloc[0]["total_cli_requests"])
                    if df.iloc[0]["total_cli_requests"]
                    else 0
                ),
                "unique_users": (
                    int(df.iloc[0]["unique_users"])
                    if df.iloc[0]["unique_users"]
                    else 0
                ),
                "active_days": (
                    int(df.iloc[0]["active_days"]) if df.iloc[0]["active_days"] else 0
                ),
            }
            logger.info(f"QCli total summary: {result}")
            return result
        else:
            logger.warning("No data found for summary")
            return {
                "total_requests": 0,
                "total_agentic_requests": 0,
                "total_cli_requests": 0,
                "unique_users": 0,
                "active_days": 0,
            }

    def get_user_usage_analysis(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì‚¬ìš©ìë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        logger.info(
            f"Getting QCli user usage analysis from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND user_id LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            user_id,
            SUM(CAST(request_count AS BIGINT)) as total_requests,
            SUM(CAST(agentic_request_count AS BIGINT)) as total_agentic_requests,
            SUM(CAST(code_suggestion_count AS BIGINT)) as total_code_suggestions,
            SUM(CAST(cli_request_count AS BIGINT)) as total_cli_requests,
            SUM(CAST(ide_request_count AS BIGINT)) as total_ide_requests,
            COUNT(DISTINCT date) as active_days,
            MIN(date) as first_activity,
            MAX(date) as last_activity
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        GROUP BY user_id
        ORDER BY total_requests DESC
        """

        return self.execute_athena_query(query)

    def get_daily_usage_pattern(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        logger.info(
            f"Getting QCli daily usage pattern from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND user_id LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            CAST(date AS VARCHAR) as date_str,
            SUM(CAST(request_count AS BIGINT)) as total_requests,
            SUM(CAST(agentic_request_count AS BIGINT)) as total_agentic_requests,
            SUM(CAST(cli_request_count AS BIGINT)) as total_cli_requests,
            COUNT(DISTINCT user_id) as unique_users
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        GROUP BY date
        ORDER BY date
        """

        return self.execute_athena_query(query)


    def get_feature_usage_stats(
        self, start_date: datetime, end_date: datetime, user_pattern: str = None
    ) -> pd.DataFrame:
        """ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„ (CLI vs IDE, Agentic vs Code Suggestion)"""
        logger.info(
            f"Getting QCli feature usage stats from {start_date} to {end_date}, user_pattern={user_pattern}"
        )

        user_filter = f"AND user_id LIKE '%{user_pattern}%'" if user_pattern else ""

        query = f"""
        SELECT
            'CLI Requests' as feature_type,
            SUM(CAST(cli_request_count AS BIGINT)) as total_count,
            COUNT(DISTINCT user_id) as unique_users
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        UNION ALL
        SELECT
            'IDE Requests' as feature_type,
            SUM(CAST(ide_request_count AS BIGINT)) as total_count,
            COUNT(DISTINCT user_id) as unique_users
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        UNION ALL
        SELECT
            'Agentic Requests' as feature_type,
            SUM(CAST(agentic_request_count AS BIGINT)) as total_count,
            COUNT(DISTINCT user_id) as unique_users
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        UNION ALL
        SELECT
            'Code Suggestions' as feature_type,
            SUM(CAST(code_suggestion_count AS BIGINT)) as total_count,
            COUNT(DISTINCT user_id) as unique_users
        FROM qcli_user_activity_reports
        WHERE date BETWEEN DATE '{start_date.strftime('%Y-%m-%d')}'
            AND DATE '{end_date.strftime('%Y-%m-%d')}'
            {user_filter}
        ORDER BY total_count DESC
        """

        return self.execute_athena_query(query)


def calculate_cost_for_dataframe(
    df: pd.DataFrame, model_col: str = "model_name", region: str = "default"
) -> pd.DataFrame:
    """DataFrameì— ë¹„ìš© ì»¬ëŸ¼ ì¶”ê°€ (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)

    Args:
        df: ë¹„ìš©ì„ ê³„ì‚°í•  DataFrame
        model_col: ëª¨ë¸ëª…ì´ ìˆëŠ” ì»¬ëŸ¼ëª…
        region: AWS ë¦¬ì „ (ì˜ˆ: us-east-1, ap-northeast-2)

    Returns:
        pd.DataFrame: ë¹„ìš© ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    logger.info(f"Calculating cost for DataFrame with {len(df)} rows, region: {region}")

    if df.empty:
        return df

    costs = []
    for _, row in df.iterrows():
        model = row.get(model_col, "")
        input_tokens = (
            int(row.get("total_input_tokens", 0))
            if row.get("total_input_tokens")
            else 0
        )
        output_tokens = (
            int(row.get("total_output_tokens", 0))
            if row.get("total_output_tokens")
            else 0
        )
        cost = get_model_cost(model, input_tokens, output_tokens, region)
        costs.append(cost)

    df["estimated_cost_usd"] = costs
    logger.info(f"Total cost calculated for region {region}: ${sum(costs):.4f}")
    return df


def main():
    logger.info("Starting Analytics Dashboard")

    st.set_page_config(
        page_title="AWS Analytics Dashboard", page_icon="ğŸ“Š", layout="wide"
    )

    st.title("ğŸ“Š AWS Analytics Dashboard")
    st.markdown("**Athena ê¸°ë°˜ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ ë¶„ì„ - Bedrock & Amazon Q CLI**")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

    # ë¶„ì„ ìœ í˜• ì„ íƒ
    analysis_type = st.sidebar.radio(
        "ë¶„ì„ ìœ í˜• ì„ íƒ",
        ["AWS Bedrock", "Amazon Q CLI"],
        index=0
    )

    # ë¦¬ì „ ì„ íƒ
    if analysis_type == "Amazon Q CLI":
        # Amazon Q CLIëŠ” us-east-1ì—ì„œë§Œ ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ ê´€ë¦¬
        st.sidebar.info("â„¹ï¸ Amazon Q CLI ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ëŠ” us-east-1ì—ì„œë§Œ ê´€ë¦¬ë©ë‹ˆë‹¤.")
        selected_region = "us-east-1"
        st.sidebar.text(f"ë¦¬ì „: {selected_region} - {REGIONS[selected_region]} (ê³ ì •)")
    else:
        # Bedrockì€ ëª¨ë“  ë¦¬ì „ ì„ íƒ ê°€ëŠ¥
        selected_region = st.sidebar.selectbox(
            "ë¦¬ì „ ì„ íƒ",
            options=list(REGIONS.keys()),
            format_func=lambda x: f"{x} - {REGIONS[x]}",
            index=4,
        )

    logger.info(f"Selected region: {selected_region}, Analysis type: {analysis_type}")

    # ë‚ ì§œ ë²”ìœ„ ì„ íƒ
    st.sidebar.subheader("ğŸ“… ë‚ ì§œ ë²”ìœ„ ì„ íƒ")

    col1, col2 = st.sidebar.columns(2)
    with col1:
        start_date = st.date_input(
            "ì‹œì‘ ë‚ ì§œ",
            value=datetime.now() - timedelta(days=7),
            max_value=datetime.now(),
        )
    with col2:
        end_date = st.date_input(
            "ì¢…ë£Œ ë‚ ì§œ", value=datetime.now(), max_value=datetime.now()
        )

    logger.info(f"Date range: {start_date} to {end_date}")

    # ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ëŒ€ì‹œë³´ë“œ ë Œë”ë§
    if analysis_type == "AWS Bedrock":
        render_bedrock_analytics(selected_region, start_date, end_date)
    else:
        render_qcli_analytics(selected_region, start_date, end_date)


def render_bedrock_analytics(selected_region, start_date, end_date):
    """Bedrock ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    logger.info("Rendering Bedrock Analytics")

    # ARN íŒ¨í„´ í•„í„°
    st.sidebar.subheader("ğŸ” ARN íŒ¨í„´ í•„í„° (ì„ íƒì‚¬í•­)")
    arn_pattern = st.sidebar.text_input(
        "ARN íŒ¨í„´",
        value="",
        placeholder="ì˜ˆ: AmazonQ-CLI, q-cli",
        key="bedrock_arn_pattern",
        help="íŠ¹ì • ARN íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ì‚¬ìš©ìë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ì‚¬ìš©ìë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
    )

    # í˜„ì¬ ë¡œê¹… ì„¤ì • ìë™ ì¡°íšŒ
    tracker = BedrockAthenaTracker(region=selected_region)

    with st.spinner("í˜„ì¬ Model Invocation Logging ì„¤ì • í™•ì¸ ì¤‘..."):
        current_config = tracker.get_current_logging_config()

    # ì„¤ì • ìƒíƒœ í‘œì‹œ
    if current_config["status"] == "enabled":
        st.success("âœ… Model Invocation Loggingì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")

        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ“ **S3 ë²„í‚·**: ì„¤ì •ë¨ ({selected_region})")
        with col2:
            st.info(f"ğŸ“‚ **í”„ë¦¬í”½ìŠ¤**: ì„¤ì •ë¨")

    elif current_config["status"] == "disabled":
        st.error("âŒ Model Invocation Loggingì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        st.markdown("ğŸ‘‡ ë¨¼ì € ì„¤ì •ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”:")
        st.code("python setup_bedrock_analytics.py")
        logger.error("Model Invocation Logging is disabled")
        return

    else:
        st.warning(
            f"âš ï¸ ì„¤ì • í™•ì¸ ì¤‘ ì˜¤ë¥˜: {current_config.get('error', 'Unknown error')}"
        )
        logger.error(f"Error checking logging config: {current_config.get('error')}")
        return

    # ë¶„ì„ ì‹¤í–‰
    if st.sidebar.button("ğŸ” ë°ì´í„° ë¶„ì„", type="primary"):
        logger.info("Analysis button clicked")

        with st.spinner("Athenaì—ì„œ ë°ì´í„° ë¶„ì„ ì¤‘..."):

            # ARN íŒ¨í„´ ì •ë³´ í‘œì‹œ
            if arn_pattern:
                st.info(f"ğŸ” ARN íŒ¨í„´ í•„í„°ë§ ì ìš©: '{arn_pattern}'")

            # ì „ì²´ ìš”ì•½
            summary = tracker.get_total_summary(start_date, end_date, arn_pattern if arn_pattern else None)

            st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ì´ API í˜¸ì¶œ", f"{summary['total_calls']:,}")

            with col2:
                st.metric("ì´ Input í† í°", f"{summary['total_input_tokens']:,}")

            with col3:
                st.metric("ì´ Output í† í°", f"{summary['total_output_tokens']:,}")

            # ëª¨ë¸ë³„ í†µê³„ë¡œ ì´ ë¹„ìš© ê³„ì‚°
            model_df = tracker.get_model_usage_stats(start_date, end_date, arn_pattern if arn_pattern else None)
            if not model_df.empty:
                model_df = calculate_cost_for_dataframe(model_df, region=selected_region)
                total_cost = model_df["estimated_cost_usd"].sum()
                summary["total_cost_usd"] = total_cost

            with col4:
                st.metric("ì´ ë¹„ìš©", f"${summary['total_cost_usd']:.4f}")

            # ì‚¬ìš©ìë³„ ë¶„ì„
            st.header("ğŸ‘¥ ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¶„ì„")

            user_df = tracker.get_user_cost_analysis(start_date, end_date, arn_pattern if arn_pattern else None)

            if not user_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(
                            user_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚°ì„ ìœ„í•œ ì„ì‹œ ëª¨ë¸ëª… ì¶”ê°€ (ëª¨ë¸ë³„ í‰ê·  ì‚¬ìš©)
                # ì‹¤ì œë¡œëŠ” ê° ì‚¬ìš©ìê°€ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í–ˆëŠ”ì§€ ì•Œì•„ì•¼ ì •í™•í•¨
                # ì—¬ê¸°ì„œëŠ” Claude 3 Haiku ê¸°ë³¸ ê°€ê²© ì‚¬ìš© (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)
                costs = []
                for _, row in user_df.iterrows():
                    input_tokens = int(row.get("total_input_tokens", 0)) if row.get("total_input_tokens") else 0
                    output_tokens = int(row.get("total_output_tokens", 0)) if row.get("total_output_tokens") else 0
                    # Claude 3 Haikuë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©
                    cost = get_model_cost("claude-3-haiku-20240307", input_tokens, output_tokens, selected_region)
                    costs.append(cost)
                user_df["estimated_cost_usd"] = costs

                st.dataframe(user_df, use_container_width=True)

                # ë¹„ìš© ì°¨íŠ¸
                if len(user_df) > 0:
                    import plotly.express as px

                    fig = px.bar(
                        user_df.head(10),
                        x="user_or_app",
                        y="estimated_cost_usd",
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¹„ìš©",
                        labels={
                            "user_or_app": "ì‚¬ìš©ì/ì• í”Œë¦¬ì¼€ì´ì…˜",
                            "estimated_cost_usd": "ë¹„ìš© (USD)",
                        },
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„
            st.header("ğŸ“± ìœ ì €ë³„ ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ìƒì„¸ ë¶„ì„")

            user_app_df = tracker.get_user_app_detail_analysis(start_date, end_date, arn_pattern if arn_pattern else None)

            if not user_app_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "total_input_tokens",
                    "total_output_tokens",
                ]
                for col in numeric_columns:
                    if col in user_app_df.columns:
                        user_app_df[col] = pd.to_numeric(
                            user_app_df[col], errors="coerce"
                        ).fillna(0)

                # ë¹„ìš© ê³„ì‚° (ë¦¬ì „ë³„ ê°€ê²© ë°˜ì˜)
                user_app_df = calculate_cost_for_dataframe(user_app_df, region=selected_region)

                st.dataframe(user_app_df, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ëª¨ë¸ë³„ ë¶„ì„
            st.header("ğŸ¤– ëª¨ë¸ë³„ ì‚¬ìš© í†µê³„")

            if not model_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "call_count",
                    "avg_input_tokens",
                    "avg_output_tokens",
                    "total_input_tokens",
                    "total_output_tokens",
                    "estimated_cost_usd",
                ]
                for col in numeric_columns:
                    if col in model_df.columns:
                        model_df[col] = pd.to_numeric(
                            model_df[col], errors="coerce"
                        ).fillna(0)

                st.dataframe(model_df, use_container_width=True)

                # ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨ ì°¨íŠ¸
                if len(model_df) > 0:
                    import plotly.express as px

                    fig = px.pie(
                        model_df,
                        values="call_count",
                        names="model_name",
                        title="ëª¨ë¸ë³„ í˜¸ì¶œ ë¹„ìœ¨",
                    )
                    st.plotly_chart(fig, use_container_width=True)

            # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
            st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

            daily_df = tracker.get_daily_usage_pattern(start_date, end_date, arn_pattern if arn_pattern else None)

            if not daily_df.empty and len(daily_df) > 0:
                # ë‚ ì§œ ì»¬ëŸ¼ ìƒì„±
                daily_df["date"] = pd.to_datetime(
                    daily_df["year"]
                    + "-"
                    + daily_df["month"].str.zfill(2)
                    + "-"
                    + daily_df["day"].str.zfill(2)
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = ["call_count", "total_input_tokens", "total_output_tokens"]
                for col in numeric_columns:
                    if col in daily_df.columns:
                        daily_df[col] = pd.to_numeric(
                            daily_df[col], errors="coerce"
                        ).fillna(0)

                # í‘œì‹œìš© DataFrame ìƒì„± (ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·)
                display_df = daily_df.copy()
                display_df["ë‚ ì§œ"] = display_df["date"].dt.strftime("%Y-%m-%d")
                display_df = display_df[["ë‚ ì§œ", "call_count", "total_input_tokens", "total_output_tokens"]]
                display_df.columns = ["ë‚ ì§œ", "API í˜¸ì¶œ ìˆ˜", "Input í† í°", "Output í† í°"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì¼ë³„ API í˜¸ì¶œ íŒ¨í„´
                fig = px.line(
                    daily_df,
                    x="date",
                    y="call_count",
                    title="ì¼ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"date": "ë‚ ì§œ", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                    markers=True
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰
                fig2 = go.Figure()
                fig2.add_trace(go.Scatter(
                    x=daily_df["date"],
                    y=daily_df["total_input_tokens"],
                    mode='lines+markers',
                    name='Input í† í°',
                    line=dict(color='blue')
                ))
                fig2.add_trace(go.Scatter(
                    x=daily_df["date"],
                    y=daily_df["total_output_tokens"],
                    mode='lines+markers',
                    name='Output í† í°',
                    line=dict(color='red')
                ))
                fig2.update_layout(
                    title="ì¼ë³„ í† í° ì‚¬ìš©ëŸ‰",
                    xaxis_title="ë‚ ì§œ",
                    yaxis_title="í† í° ìˆ˜",
                    hovermode='x unified'
                )
                st.plotly_chart(fig2, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì¼ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            st.header("â° ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´")

            hourly_df = tracker.get_hourly_usage_pattern(start_date, end_date, arn_pattern if arn_pattern else None)

            if not hourly_df.empty and len(hourly_df) > 0:
                # ì‹œê°„ ì»¬ëŸ¼ ìƒì„±
                hourly_df["datetime"] = pd.to_datetime(
                    hourly_df["year"]
                    + "-"
                    + hourly_df["month"].str.zfill(2)
                    + "-"
                    + hourly_df["day"].str.zfill(2)
                    + " "
                    + hourly_df["hour"].str.zfill(2)
                    + ":00:00"
                )

                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = ["call_count", "total_input_tokens", "total_output_tokens"]
                for col in numeric_columns:
                    if col in hourly_df.columns:
                        hourly_df[col] = pd.to_numeric(
                            hourly_df[col], errors="coerce"
                        ).fillna(0)

                # í‘œì‹œìš© DataFrame ìƒì„±
                display_df = hourly_df.copy()
                display_df["ì‹œê°„"] = display_df["datetime"].dt.strftime("%Y-%m-%d %H:00")
                display_df = display_df[["ì‹œê°„", "call_count", "total_input_tokens", "total_output_tokens"]]
                display_df.columns = ["ì‹œê°„", "API í˜¸ì¶œ ìˆ˜", "Input í† í°", "Output í† í°"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì‹œê°„ëŒ€ë³„ API í˜¸ì¶œ íŒ¨í„´
                fig = px.line(
                    hourly_df,
                    x="datetime",
                    y="call_count",
                    title="ì‹œê°„ëŒ€ë³„ API í˜¸ì¶œ íŒ¨í„´",
                    labels={"datetime": "ì‹œê°„", "call_count": "API í˜¸ì¶œ ìˆ˜"},
                    markers=True
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì‹œê°„ëŒ€ë³„ í† í° ì‚¬ìš©ëŸ‰
                fig2 = go.Figure()
                fig2.add_trace(go.Scatter(
                    x=hourly_df["datetime"],
                    y=hourly_df["total_input_tokens"],
                    mode='lines+markers',
                    name='Input í† í°',
                    line=dict(color='blue')
                ))
                fig2.add_trace(go.Scatter(
                    x=hourly_df["datetime"],
                    y=hourly_df["total_output_tokens"],
                    mode='lines+markers',
                    name='Output í† í°',
                    line=dict(color='red')
                ))
                fig2.update_layout(
                    title="ì‹œê°„ëŒ€ë³„ í† í° ì‚¬ìš©ëŸ‰",
                    xaxis_title="ì‹œê°„",
                    yaxis_title="í† í° ìˆ˜",
                    hovermode='x unified'
                )
                st.plotly_chart(fig2, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì‹œê°„ëŒ€ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        # ì´ˆê¸° í™”ë©´
        st.info(
            "ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¦¬ì „ê³¼ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•œ í›„ 'ë°ì´í„° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
        )

        st.markdown("### ğŸ› ï¸ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ")

        st.markdown("#### 1ï¸âƒ£ í™˜ê²½ ìš”êµ¬ì‚¬í•­")
        st.markdown(
            """
        **AWS ê¶Œí•œ**: ë‹¤ìŒ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤
        - Bedrock: InvokeModel, Get/PutModelInvocationLoggingConfiguration
        - S3: GetObject, ListBucket, PutObject, CreateBucket
        - Athena: StartQueryExecution, GetQueryExecution, GetQueryResults
        - Glue: CreateDatabase, CreateTable, GetDatabase, GetTable

        **Python í™˜ê²½**:
        - Python 3.8 ì´ìƒ
        - boto3, streamlit, pandas, plotly
        """
        )

        st.markdown("#### 2ï¸âƒ£ ì„¤ì¹˜ ë°©ë²•")
        st.code(
            """
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 2. AWS ìê²©ì¦ëª… ì„¤ì •
aws configure
# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=us-east-1
        """,
            language="bash"
        )

        st.markdown("#### 3ï¸âƒ£ ì´ˆê¸° ì„¤ì • ë‹¨ê³„")
        st.code(
            """
# Step 1: Athena ë¶„ì„ í™˜ê²½ êµ¬ì¶•
python setup_athena_bucket.py

# Step 2: Bedrock ë¡œê¹… ì„¤ì • í™•ì¸ ë° í™œì„±í™”
python check_bedrock_logging.py
python setup_bedrock_logging.py

# Step 3: IAM Role ê¶Œí•œ ê²€ì¦
python verify_bedrock_permissions.py

# Step 4: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì„ íƒì‚¬í•­)
python generate_test_data.py

# Step 5: ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
streamlit run bedrock_tracker.py
        """,
            language="bash"
        )

        st.markdown("### ğŸ“‹ ì§€ì› ëª¨ë¸")
        st.markdown(
            """
        - **Claude 3**: Haiku, Sonnet, Opus
        - **Claude 3.5**: Haiku, Sonnet
        - **Claude 3.7**: Sonnet
        - **Claude 4**: Sonnet 4, Sonnet 4.5
        - **Claude 4.1**: Opus
        """
        )

        st.markdown("### ğŸŒ ì§€ì› ë¦¬ì „")
        for region_id, region_name in REGIONS.items():
            st.markdown(f"- **{region_id}**: {region_name}")

    logger.info("Bedrock Dashboard rendering complete")


def render_qcli_analytics(selected_region, start_date, end_date):
    """Amazon Q CLI ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    logger.info("Rendering Amazon Q CLI Analytics")

    # ì‚¬ìš©ì íŒ¨í„´ í•„í„°
    st.sidebar.subheader("ğŸ” ì‚¬ìš©ì ID í•„í„° (ì„ íƒì‚¬í•­)")
    user_pattern = st.sidebar.text_input(
        "ì‚¬ìš©ì ID íŒ¨í„´",
        value="",
        placeholder="ì˜ˆ: user@example.com",
        key="qcli_user_pattern",
        help="íŠ¹ì • ì‚¬ìš©ì ID íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ì‚¬ìš©ìë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ì‚¬ìš©ìë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
    )

    # QCli Tracker ì´ˆê¸°í™”
    tracker = QCliAthenaTracker(region=selected_region)

    # ì´ˆê¸° ì •ë³´ í‘œì‹œ
    st.info(
        "ğŸ“‹ **Amazon Q CLI ì‚¬ìš©ëŸ‰ ë¶„ì„**\n\n"
        "ì´ ëŒ€ì‹œë³´ë“œëŠ” Amazon Q Developerì˜ ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ CSV íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n"
        "CSV ë¦¬í¬íŠ¸ëŠ” ë§¤ì¼ ìì •(UTC)ì— ìƒì„±ë˜ë©° S3 ë²„í‚·ì— ì €ì¥ë©ë‹ˆë‹¤."
    )

    # ë¶„ì„ ì‹¤í–‰
    if st.sidebar.button("ğŸ” ë°ì´í„° ë¶„ì„", type="primary", key="qcli_analyze"):
        logger.info("QCli Analysis button clicked")

        with st.spinner("Athenaì—ì„œ Amazon Q CLI ë°ì´í„° ë¶„ì„ ì¤‘..."):

            # ì‚¬ìš©ì íŒ¨í„´ ì •ë³´ í‘œì‹œ
            if user_pattern:
                st.info(f"ğŸ” ì‚¬ìš©ì ID íŒ¨í„´ í•„í„°ë§ ì ìš©: '{user_pattern}'")

            # ì „ì²´ ìš”ì•½
            summary = tracker.get_total_summary(
                start_date, end_date, user_pattern if user_pattern else None
            )

            st.header("ğŸ“Š ì „ì²´ ìš”ì•½")

            col1, col2, col3, col4, col5 = st.columns(5)

            with col1:
                st.metric("ì´ ìš”ì²­ ìˆ˜", f"{summary['total_requests']:,}")

            with col2:
                st.metric("Agentic ìš”ì²­", f"{summary['total_agentic_requests']:,}")

            with col3:
                st.metric("CLI ìš”ì²­", f"{summary['total_cli_requests']:,}")

            with col4:
                st.metric("í™œì„± ì‚¬ìš©ì", f"{summary['unique_users']:,}")

            with col5:
                st.metric("í™œë™ ì¼ìˆ˜", f"{summary['active_days']:,}")

            # ì‚¬ìš©ìë³„ ë¶„ì„
            st.header("ğŸ‘¥ ì‚¬ìš©ìë³„ ë¶„ì„")

            user_df = tracker.get_user_usage_analysis(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not user_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "total_requests",
                    "total_agentic_requests",
                    "total_code_suggestions",
                    "total_cli_requests",
                    "total_ide_requests",
                    "active_days",
                ]
                for col in numeric_columns:
                    if col in user_df.columns:
                        user_df[col] = pd.to_numeric(user_df[col], errors="coerce").fillna(0)

                st.dataframe(user_df, use_container_width=True)

                # ì‚¬ìš©ìë³„ ìš”ì²­ ìˆ˜ ì°¨íŠ¸
                if len(user_df) > 0:
                    import plotly.express as px

                    fig = px.bar(
                        user_df.head(10),
                        x="user_id",
                        y="total_requests",
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ìë³„ ì´ ìš”ì²­ ìˆ˜",
                        labels={"user_id": "ì‚¬ìš©ì ID", "total_requests": "ì´ ìš”ì²­ ìˆ˜"},
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)

                    # Agentic vs Non-Agentic ìš”ì²­ ë¹„êµ
                    fig2 = px.bar(
                        user_df.head(10),
                        x="user_id",
                        y=["total_agentic_requests", "total_code_suggestions"],
                        title="ìƒìœ„ 10ëª… ì‚¬ìš©ìë³„ ìš”ì²­ ìœ í˜• ë¶„í¬",
                        labels={"value": "ìš”ì²­ ìˆ˜", "user_id": "ì‚¬ìš©ì ID"},
                        barmode="group",
                    )
                    fig2.update_xaxes(tickangle=45)
                    st.plotly_chart(fig2, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ì‚¬ìš©ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„
            st.header("ğŸ“± ê¸°ëŠ¥ë³„ ì‚¬ìš© í†µê³„")

            feature_df = tracker.get_feature_usage_stats(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not feature_df.empty:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                for col in ["total_count", "unique_users"]:
                    if col in feature_df.columns:
                        feature_df[col] = pd.to_numeric(feature_df[col], errors="coerce").fillna(0)

                st.dataframe(feature_df, use_container_width=True)

                # ê¸°ëŠ¥ë³„ ì‚¬ìš©ëŸ‰ íŒŒì´ ì°¨íŠ¸
                if len(feature_df) > 0:
                    import plotly.express as px

                    fig = px.pie(
                        feature_df,
                        values="total_count",
                        names="feature_type",
                        title="ê¸°ëŠ¥ë³„ ì‚¬ìš© ë¹„ìœ¨",
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ë¶„ì„í•  ê¸°ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì¼ë³„ ì‚¬ìš© íŒ¨í„´
            st.header("ğŸ“… ì¼ë³„ ì‚¬ìš© íŒ¨í„´")

            daily_df = tracker.get_daily_usage_pattern(
                start_date, end_date, user_pattern if user_pattern else None
            )

            if not daily_df.empty and len(daily_df) > 0:
                # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                numeric_columns = [
                    "total_requests",
                    "total_agentic_requests",
                    "total_cli_requests",
                    "unique_users",
                ]
                for col in numeric_columns:
                    if col in daily_df.columns:
                        daily_df[col] = pd.to_numeric(daily_df[col], errors="coerce").fillna(0)

                # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                daily_df["date"] = pd.to_datetime(daily_df["date_str"])

                # í‘œì‹œìš© DataFrame ìƒì„±
                display_df = daily_df.copy()
                display_df["ë‚ ì§œ"] = display_df["date"].dt.strftime("%Y-%m-%d")
                display_df = display_df[
                    ["ë‚ ì§œ", "total_requests", "total_agentic_requests", "total_cli_requests", "unique_users"]
                ]
                display_df.columns = ["ë‚ ì§œ", "ì´ ìš”ì²­ ìˆ˜", "Agentic ìš”ì²­", "CLI ìš”ì²­", "í™œì„± ì‚¬ìš©ì"]

                # 1. í…Œì´ë¸” ë¨¼ì € í‘œì‹œ
                st.dataframe(display_df, use_container_width=True)

                # 2. ê·¸ë˜í”„ í‘œì‹œ
                import plotly.express as px
                import plotly.graph_objects as go

                # ì¼ë³„ ìš”ì²­ íŒ¨í„´
                fig = px.line(
                    daily_df,
                    x="date",
                    y="total_requests",
                    title="ì¼ë³„ ì´ ìš”ì²­ ìˆ˜",
                    labels={"date": "ë‚ ì§œ", "total_requests": "ì´ ìš”ì²­ ìˆ˜"},
                    markers=True,
                )
                st.plotly_chart(fig, use_container_width=True)

                # ì¼ë³„ ìš”ì²­ ìœ í˜•ë³„ ë¶„í¬
                fig2 = go.Figure()
                fig2.add_trace(
                    go.Scatter(
                        x=daily_df["date"],
                        y=daily_df["total_agentic_requests"],
                        mode="lines+markers",
                        name="Agentic ìš”ì²­",
                        line=dict(color="blue"),
                    )
                )
                fig2.add_trace(
                    go.Scatter(
                        x=daily_df["date"],
                        y=daily_df["total_cli_requests"],
                        mode="lines+markers",
                        name="CLI ìš”ì²­",
                        line=dict(color="green"),
                    )
                )
                fig2.update_layout(
                    title="ì¼ë³„ ìš”ì²­ ìœ í˜•ë³„ ë¶„í¬",
                    xaxis_title="ë‚ ì§œ",
                    yaxis_title="ìš”ì²­ ìˆ˜",
                    hovermode="x unified",
                )
                st.plotly_chart(fig2, use_container_width=True)

                # ì¼ë³„ í™œì„± ì‚¬ìš©ì ìˆ˜
                fig3 = px.bar(
                    daily_df,
                    x="date",
                    y="unique_users",
                    title="ì¼ë³„ í™œì„± ì‚¬ìš©ì ìˆ˜",
                    labels={"date": "ë‚ ì§œ", "unique_users": "í™œì„± ì‚¬ìš©ì ìˆ˜"},
                )
                st.plotly_chart(fig3, use_container_width=True)
            else:
                st.warning("ì„ íƒí•œ ê¸°ê°„ì— ì¼ë³„ ì‚¬ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        # ì´ˆê¸° í™”ë©´
        st.info(
            "ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¦¬ì „ê³¼ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•œ í›„ 'ë°ì´í„° ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
        )

        st.markdown("### ğŸ› ï¸ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ")

        st.markdown("#### 1ï¸âƒ£ Amazon Q Developer ì„¤ì •")
        st.markdown(
            """
        1. **Amazon Q Developer ì½˜ì†”**ì—ì„œ "Collect granular metrics per user" ì˜µì…˜ í™œì„±í™”
        2. **S3 ë²„í‚· ì§€ì •**: ì‚¬ìš©ì í™œë™ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë  S3 ë²„í‚· ì„¤ì •
        3. **ë§¤ì¼ ìì •(UTC)**ì— CSV ë¦¬í¬íŠ¸ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
        """
        )

        st.markdown("#### 2ï¸âƒ£ ë¶„ì„ í™˜ê²½ êµ¬ì¶•")
        st.code(
            """
# Amazon Q CLI ë¶„ì„ í™˜ê²½ ì„¤ì •
python setup_qcli_analytics.py --region us-east-1

# ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
streamlit run bedrock_tracker.py
        """,
            language="bash",
        )

        st.markdown("#### 3ï¸âƒ£ ë°ì´í„° ì†ŒìŠ¤")
        st.markdown(
            """
        **CSV ë¦¬í¬íŠ¸ (ì‚¬ìš©ì í™œë™)**:
        - ì¼ë³„ ì‚¬ìš©ìë³„ ìš”ì²­ ìˆ˜
        - Agentic ìš”ì²­ ìˆ˜
        - CLI/IDE ìš”ì²­ ìˆ˜
        - ì½”ë“œ ì œì•ˆ ìˆ˜
        """
        )

        st.markdown("### ğŸ“‹ ì£¼ìš” ë©”íŠ¸ë¦­")
        st.markdown(
            """
        - **ì´ ìš”ì²­ ìˆ˜**: ì „ì²´ Amazon Q ìš”ì²­ ìˆ˜
        - **Agentic ìš”ì²­**: Q&A ì±— ë˜ëŠ” agentic ì½”ë”© ìƒí˜¸ì‘ìš©
        - **CLI ìš”ì²­**: Amazon Q CLIë¥¼ í†µí•œ ìš”ì²­
        - **IDE ìš”ì²­**: IDE í”ŒëŸ¬ê·¸ì¸ì„ í†µí•œ ìš”ì²­
        - **ì½”ë“œ ì œì•ˆ**: ì½”ë“œ ìë™ ì™„ì„± ì œì•ˆ ìˆ˜
        """
        )

    logger.info("QCli Dashboard rendering complete")


if __name__ == "__main__":
    main()
